dataset:
  filename:          "energy_data.csv"
  train_valid_split:  0.8  # 80% training 20% validation

autoencoder_design:
  hidden_dim:         22   # layer 1 nodes
  encoding_dim:       10   # layer 2 nodes
  latent_dim:         6    # latent layer nodes
  dropout_prob:       0.15 # probability of dropping units during training

autoencoder_training:
  batch_size:         32        # number of samples per batch
  training_epochs:    30        # training epochs
  training_patience:  5         # how many epochs with no improvement to stop training
  optimizer_lr:       0.0001    # learning rate for the optimizer
  weight_decay:       0.00001   # for L2 regularization
  scheduler_patience: 3
  scheduler_mode:     'min'     # min (max) reduces elarning rate when validation loss stops improving (starts increasing)
  scheduler_factor:   0.8       # multiplies lr by this factor when validation loss plateaus

diffusion_design:
  diff_steps:         30        # num of diffusion steps

noise_scheduler:                # for diffusion
  noise_profile:      'l'       # l = linear, c = cos, q = quadratic
  cos_start_offset:   0.008     # small offset that smooths the early part of cos schedule (avoid instability or extreme vals)
  start_noise_val:    0.0001    # start noise value, non-0 as it can create NaN values
  end_noise_val:      0.02      # end noise value

diffusion_training:
  train_epochs:       100       # num of diffusion training epochs
  training_patience:  10        # how many epochs with no improvement to stop training
  optimizer_lr:       0.0005    # learning rate for the optimizer
  weight_decay:       0.0000001 # for L2 regularization
  scheduler_patience: 3
  scheduler_mode:     'min'     # min (max) reduces elarning rate when validation loss stops improving (starts increasing)
  scheduler_factor:   0.5       # multiplies lr by this factor when validation loss plateaus

Unet_design:
  dropout_prob:       0.1  # probability of dropping units during training
  base_channels:      18   # initial # of channels in 1st convolution layer
  embedding_dim:      128  # time-step embedding size

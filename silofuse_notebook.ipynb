{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:30:20.077373Z",
     "start_time": "2025-03-20T13:30:20.001071Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "# sys.path.append(os.path.join(os.getcwd(), 'detection_copied_from_timewak'))\n",
    "sys.path.append(os.path.abspath('.')) # to run files that are away\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"  # Suppress WandB logs\n",
    "\n",
    "libraries = [\"torch\", \"numpy\", \"polars\"]\n",
    "modules   = {lib: sys.modules.get(lib) for lib in libraries}\n",
    "\n",
    "if not modules[\"torch\"]:\n",
    "    import torch\n",
    "if not modules[\"numpy\"]:\n",
    "    import numpy as np\n",
    "if not modules[\"polars\"]:\n",
    "    import polars as pl\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from autoencoder import Autoencoder, train_autoencoder, compute_reconstruction_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loading the data\"\"\"\n",
    "\n",
    "with open('params.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "data_file  = f\"./datasets/{config['dataset']['filename']}\"\n",
    "df         = pd.read_csv(data_file)\n",
    "\n",
    "X          = MinMaxScaler().fit_transform(df.values)\n",
    "X_tensor   = torch.tensor(X, dtype=torch.float32)\n",
    "dataset    = TensorDataset(X_tensor, X_tensor)  # Autoencoder reconstructs the input\n",
    "train_size = int(config['dataset']['train_valid_split'] * len(df))\n",
    "val_size   = len(df) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], training loss: 0.0323\n",
      "Validation loss: 0.0144\n",
      "Epoch [2/30], training loss: 0.0101\n",
      "Validation loss: 0.0078\n",
      "Epoch [3/30], training loss: 0.0077\n",
      "Validation loss: 0.0076\n",
      "Epoch [4/30], training loss: 0.0073\n",
      "Validation loss: 0.0067\n",
      "Epoch [5/30], training loss: 0.0061\n",
      "Validation loss: 0.0059\n",
      "Epoch [6/30], training loss: 0.0059\n",
      "Validation loss: 0.0057\n",
      "Epoch [7/30], training loss: 0.0058\n",
      "Validation loss: 0.0058\n",
      "Epoch [8/30], training loss: 0.0057\n",
      "Validation loss: 0.0056\n",
      "Epoch [9/30], training loss: 0.0057\n",
      "Validation loss: 0.0057\n",
      "Epoch [10/30], training loss: 0.0056\n",
      "Validation loss: 0.0056\n",
      "Epoch [11/30], training loss: 0.0055\n",
      "Validation loss: 0.0057\n",
      "Epoch [12/30], training loss: 0.0055\n",
      "Validation loss: 0.0056\n",
      "Epoch [13/30], training loss: 0.0054\n",
      "Validation loss: 0.0055\n",
      "Epoch [14/30], training loss: 0.0054\n",
      "Validation loss: 0.0055\n",
      "Epoch [15/30], training loss: 0.0054\n",
      "Validation loss: 0.0054\n",
      "Epoch [16/30], training loss: 0.0053\n",
      "Validation loss: 0.0054\n",
      "Epoch [17/30], training loss: 0.0053\n",
      "Validation loss: 0.0053\n",
      "Epoch [18/30], training loss: 0.0053\n",
      "Validation loss: 0.0052\n",
      "Epoch [19/30], training loss: 0.0053\n",
      "Validation loss: 0.0054\n",
      "Epoch [20/30], training loss: 0.0052\n",
      "Validation loss: 0.0052\n",
      "Epoch [21/30], training loss: 0.0052\n",
      "Validation loss: 0.0051\n",
      "Epoch [22/30], training loss: 0.0052\n",
      "Validation loss: 0.0051\n",
      "Epoch [23/30], training loss: 0.0052\n",
      "Validation loss: 0.0052\n",
      "Epoch [24/30], training loss: 0.0052\n",
      "Validation loss: 0.0056\n",
      "Epoch [25/30], training loss: 0.0052\n",
      "Validation loss: 0.0052\n",
      "Epoch [26/30], training loss: 0.0052\n",
      "Validation loss: 0.0051\n",
      "Early stopping triggered\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Autoencoder training\"\"\"\n",
    "\n",
    "hidden_dim      = config['autoencoder_design']['hidden_dim']\n",
    "encoding_dim    = config['autoencoder_design']['encoding_dim']\n",
    "latent_dim      = config['autoencoder_design']['latent_dim']\n",
    "dropout_prob    = config['autoencoder_design']['dropout_prob']\n",
    "\n",
    "training_epochs = config['autoencoder_training']['training_epochs']\n",
    "batch_size      = config['autoencoder_training']['batch_size']\n",
    "optimizer_lr    = config['autoencoder_training']['optimizer_lr']\n",
    "weight_decay    = config['autoencoder_training']['weight_decay']\n",
    "ae_training_patience = config['autoencoder_training']['training_patience']\n",
    "\n",
    "\n",
    "# Create DataLoader objects for train and validation datasets\n",
    "input_size   = X.shape[1]\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "autoencoder  = Autoencoder(input_size, hidden_dim, encoding_dim, latent_dim, dropout_prob)\n",
    "optimizer    = torch.optim.AdamW(autoencoder.parameters(), lr=optimizer_lr, weight_decay=weight_decay)\n",
    "\n",
    "train_autoencoder(autoencoder, training_epochs, train_loader, optimizer,\n",
    "                  validation_loader=val_loader, patience = ae_training_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode (disables dropout)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# No gradient computation during inference\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     x_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnew_data\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# New data input\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     x_reconstructed \u001b[38;5;241m=\u001b[39m autoencoder(x_input)  \u001b[38;5;66;03m# Reconstruct input\u001b[39;00m\n\u001b[1;32m     10\u001b[0m reconstruction_error \u001b[38;5;241m=\u001b[39m compute_reconstruction_loss(x_input, x_reconstructed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'new_data' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"Run AE on new dataset\n",
    "Can be for reconstruction, anomaly detection, feature extraction, classification\"\"\"\n",
    "\n",
    "autoencoder.eval()  # Set the model to evaluation mode (disables dropout)\n",
    "\n",
    "with torch.no_grad():  # No gradient computation during inference\n",
    "    x_input = torch.tensor(new_data, dtype=torch.float32)  # New data input\n",
    "    x_reconstructed = autoencoder(x_input)  # Reconstruct input\n",
    "\n",
    "reconstruction_error = compute_reconstruction_loss(x_input, x_reconstructed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2., 32.,  4.],\n",
      "        [ 2.,  8.,  4.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# Diffusion model\n",
    "\n",
    "# Train Model\n",
    "#     Sample t, add noise → z_t\n",
    "#     Predict ε̂ = model(z_t, t)\n",
    "#     Loss = MSE(ε, ε̂)\n",
    "\n",
    "# Sampling\n",
    "#     Start from noise z_T\n",
    "#     Iteratively denoise to get z₀\n",
    "#     Decode with autoencoder to get time series\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "t = 4 #timestep (from loop)\n",
    "\n",
    "z_latent   = torch.tensor([[1, 2., 32., 4],[ 2, 8, 4, 6]])\n",
    "timesteps  = 1000  # total diffusion steps\n",
    "noise_betas= torch.linspace(1e-4, 0.02, timesteps)\n",
    "\n",
    "alphas     = 1.0 - noise_betas\n",
    "alpha_bars = torch.cumprod(alphas, dim=0)\n",
    "alpha_bar_t= alpha_bars[t]\n",
    "\n",
    "eps_noise  = torch.randn_like(z_latent)\n",
    "z_t        = (alpha_bar_t.sqrt() * z_latent) + ((1 - alpha_bar_t).sqrt() * eps_noise)\n",
    "eps_hat    = (z_t - alpha_bar_t.sqrt() * z_latent) / (1 - alpha_bar_t).sqrt()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

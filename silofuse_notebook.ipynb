{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:30:20.077373Z",
     "start_time": "2025-03-20T13:30:20.001071Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'detection_copied_from_timewak'))\n",
    "sys.path.append(os.path.abspath('.')) # to run files that are away\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\"  # Suppress WandB logs\n",
    "\n",
    "libraries = [\"torch\", \"numpy\", \"pandas\"]\n",
    "modules   = {lib: sys.modules.get(lib) for lib in libraries}\n",
    "\n",
    "if not modules[\"torch\"]:\n",
    "    import torch\n",
    "    # from torch import tensor, zeros, isnan, abs, fft, std, float32 as torch_float32\n",
    "if not modules[\"numpy\"]:\n",
    "    import numpy as np\n",
    "if not modules[\"pandas\"]:\n",
    "    import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4533, 0.5226, 0.4885, 0.5352, 0.5032, 0.4682, 0.5345, 0.5397, 0.4910,\n",
       "        0.4823, 0.4890, 0.4883, 0.5011, 0.5083, 0.5153, 0.5162, 0.5015, 0.4690,\n",
       "        0.5122, 0.5247, 0.4992, 0.4816, 0.5050, 0.4934, 0.4893, 0.4830, 0.4978,\n",
       "        0.5337, 0.5432, 0.5101, 0.5237, 0.4969, 0.5267, 0.5058, 0.5174, 0.5052,\n",
       "        0.4824, 0.5352, 0.5422, 0.4736, 0.5349, 0.5278, 0.4953, 0.5120, 0.4792,\n",
       "        0.5055, 0.5156, 0.4964, 0.4983, 0.4816, 0.4617, 0.5474, 0.4900, 0.5106,\n",
       "        0.4675, 0.5064, 0.5321, 0.5215, 0.5245, 0.4651, 0.5052, 0.4802, 0.4836,\n",
       "        0.5033, 0.5526, 0.4942, 0.4981, 0.4891, 0.5011, 0.5117, 0.5227, 0.5215,\n",
       "        0.5099, 0.4855, 0.5110, 0.5171, 0.5213, 0.5013, 0.4901, 0.4369, 0.4874,\n",
       "        0.4757, 0.4852, 0.5261, 0.4771, 0.4918, 0.5104, 0.4563, 0.4966, 0.4733,\n",
       "        0.5058, 0.5047, 0.5182, 0.5076, 0.4973, 0.5181, 0.5000, 0.5039, 0.5169,\n",
       "        0.5220, 0.4999, 0.5005, 0.4712, 0.4538, 0.4928, 0.4894, 0.5018, 0.4861,\n",
       "        0.4725, 0.4809, 0.4599, 0.4985, 0.4417, 0.5175, 0.4843, 0.5283, 0.5098,\n",
       "        0.5059, 0.4922, 0.5247, 0.4742, 0.5215, 0.4951, 0.5028, 0.5060, 0.5037,\n",
       "        0.5134, 0.4614, 0.4641, 0.4730, 0.5049, 0.4851, 0.4771, 0.5015, 0.5127,\n",
       "        0.5264, 0.5224, 0.5183, 0.4829, 0.4837, 0.5026, 0.5356, 0.5255, 0.5253,\n",
       "        0.4853, 0.4882, 0.5294, 0.5182, 0.5061, 0.5116, 0.4549, 0.4787, 0.5104,\n",
       "        0.5333, 0.5003, 0.4900, 0.5241, 0.5023, 0.4848, 0.4886, 0.5254, 0.4997,\n",
       "        0.4972, 0.4914, 0.4784, 0.5185, 0.5357, 0.5227, 0.5241, 0.4971, 0.4852,\n",
       "        0.5102, 0.5107, 0.4793, 0.4509, 0.4720, 0.5125, 0.4982, 0.4857, 0.5053,\n",
       "        0.4836, 0.4969, 0.4989, 0.4996, 0.5015, 0.4787, 0.4847, 0.4742, 0.4828,\n",
       "        0.5023, 0.4963, 0.5272, 0.4991, 0.4867, 0.4836, 0.4922, 0.4550, 0.4921,\n",
       "        0.5050, 0.4605, 0.4980, 0.4932, 0.4965, 0.4650, 0.4761, 0.4686, 0.5334,\n",
       "        0.5060, 0.5031, 0.5059, 0.4855, 0.5055, 0.5018, 0.5013, 0.5199, 0.5045,\n",
       "        0.5117, 0.5078, 0.4764, 0.4536, 0.5073, 0.4816, 0.4657, 0.4989, 0.5307,\n",
       "        0.4683, 0.4852, 0.4889, 0.5106, 0.5278, 0.5234, 0.4857, 0.5188, 0.4848,\n",
       "        0.5385, 0.4761, 0.4858, 0.5122, 0.5372, 0.5057, 0.5143, 0.5153, 0.5323,\n",
       "        0.5334, 0.5075, 0.4595, 0.4972, 0.5271, 0.4914, 0.5243, 0.4910, 0.4598,\n",
       "        0.4629, 0.4876, 0.5028, 0.5304, 0.4752, 0.4704, 0.4955, 0.5035, 0.4968,\n",
       "        0.4865, 0.5081, 0.5035, 0.5299, 0.5082, 0.5278, 0.4718, 0.4827, 0.4863,\n",
       "        0.4795, 0.5157, 0.5258, 0.5032, 0.5099, 0.5258, 0.5313, 0.4681, 0.4856,\n",
       "        0.5006, 0.4961, 0.5008, 0.4920, 0.4838, 0.5214, 0.5120, 0.4648, 0.5242,\n",
       "        0.5072, 0.5091, 0.4962, 0.4824, 0.4925, 0.5029, 0.5184, 0.5103, 0.5017,\n",
       "        0.4957, 0.4746, 0.4720, 0.5265, 0.4921, 0.5032, 0.5091, 0.5073, 0.4732,\n",
       "        0.4779, 0.5251, 0.5097, 0.5299, 0.5103, 0.5025, 0.5215, 0.4923, 0.4938,\n",
       "        0.5055, 0.5195, 0.4986, 0.4856, 0.5058, 0.5045, 0.5261, 0.4562, 0.4858,\n",
       "        0.4827, 0.4713, 0.5321, 0.5036, 0.5180, 0.4881, 0.4967, 0.5083, 0.4899,\n",
       "        0.4868, 0.4649, 0.5239, 0.5196, 0.4889, 0.5114, 0.5320, 0.4890, 0.4773,\n",
       "        0.5526, 0.4711, 0.4882, 0.4770, 0.5031, 0.5345, 0.5294, 0.4953, 0.5241,\n",
       "        0.4905, 0.4894, 0.4822, 0.5279, 0.4956, 0.4772, 0.5106, 0.5073, 0.4897,\n",
       "        0.5394, 0.5149, 0.5108, 0.5460, 0.4805, 0.5485, 0.5263, 0.4954, 0.5332,\n",
       "        0.5041, 0.4736, 0.4821, 0.4719, 0.4702, 0.4810, 0.5284, 0.4873, 0.4797,\n",
       "        0.4908, 0.4806, 0.5310, 0.5044, 0.4814, 0.4966, 0.4956, 0.5418, 0.4644,\n",
       "        0.5031, 0.4791, 0.5062, 0.5238, 0.5043, 0.4754, 0.4853, 0.5052, 0.5370,\n",
       "        0.4748, 0.4757, 0.4925, 0.5046, 0.5175, 0.5052, 0.4911, 0.4992, 0.4936,\n",
       "        0.5175, 0.4698, 0.5084, 0.5040, 0.5063, 0.4843, 0.4984, 0.5170, 0.4956,\n",
       "        0.5291, 0.4671, 0.5090, 0.4902, 0.4995, 0.4880, 0.5136, 0.5330, 0.4914,\n",
       "        0.4919, 0.5101, 0.4915, 0.4791, 0.5030, 0.4794, 0.4796, 0.5305, 0.4741,\n",
       "        0.4987, 0.4953, 0.4954, 0.4807, 0.4583, 0.4908, 0.4935, 0.4637, 0.4992,\n",
       "        0.5164, 0.5454, 0.4743, 0.5111, 0.5263, 0.4932, 0.4689, 0.4914, 0.5123,\n",
       "        0.4916, 0.4854, 0.4958, 0.5513, 0.5272, 0.5109, 0.5281, 0.4975, 0.4428,\n",
       "        0.5027, 0.5089, 0.5142, 0.4960, 0.4795, 0.4724, 0.5074, 0.5029, 0.4961,\n",
       "        0.4549, 0.4688, 0.5137, 0.5014, 0.4926, 0.5230, 0.4987, 0.4687, 0.5138,\n",
       "        0.5043, 0.4997, 0.5073, 0.5384, 0.4882, 0.5531, 0.4914, 0.5419, 0.5017,\n",
       "        0.4945, 0.5240, 0.5102, 0.5182, 0.5138, 0.5172, 0.5324, 0.5304, 0.4822,\n",
       "        0.5397, 0.5206, 0.5146, 0.5235, 0.5095, 0.5014, 0.4979, 0.4591, 0.5110,\n",
       "        0.5013, 0.4985, 0.5324, 0.5048, 0.4911, 0.5397, 0.4732, 0.4702, 0.5099,\n",
       "        0.5125, 0.4926, 0.4863, 0.4645, 0.4984, 0.5233, 0.5026, 0.5086, 0.4904,\n",
       "        0.4659, 0.4851, 0.5122, 0.5047, 0.5117, 0.4905, 0.5221, 0.5010, 0.5004,\n",
       "        0.5166, 0.5034, 0.4866, 0.4932, 0.4740, 0.4916, 0.5206, 0.5035, 0.5189,\n",
       "        0.5114, 0.5129, 0.5234, 0.4658, 0.5097, 0.5060, 0.5174, 0.5364, 0.4890,\n",
       "        0.5016, 0.4917, 0.4756, 0.5150, 0.5192, 0.4888, 0.4672, 0.5109, 0.4939,\n",
       "        0.4940, 0.4890, 0.5169, 0.4919, 0.4957, 0.5236, 0.5035, 0.4866, 0.5022,\n",
       "        0.5156, 0.5013, 0.4947, 0.5234, 0.4683, 0.5364, 0.5199, 0.4863, 0.4961,\n",
       "        0.4994, 0.4835, 0.5270, 0.4980, 0.4709, 0.5233, 0.5001, 0.5272, 0.4988,\n",
       "        0.4872, 0.5019, 0.4745, 0.4934, 0.4615, 0.4903, 0.5209, 0.5242, 0.5173,\n",
       "        0.4813, 0.4728, 0.4793, 0.5149, 0.5131, 0.4693, 0.4485, 0.4639, 0.4619,\n",
       "        0.4994, 0.5035, 0.5536, 0.5192, 0.5242, 0.5325, 0.5280, 0.5051, 0.5109,\n",
       "        0.4602, 0.4767, 0.4823, 0.4720, 0.5282, 0.4965, 0.4394, 0.4824, 0.4986,\n",
       "        0.5257, 0.4807, 0.5078, 0.4602, 0.5567, 0.5031, 0.4725, 0.5107, 0.4765,\n",
       "        0.5059, 0.4936, 0.5213, 0.4593, 0.4848, 0.4842, 0.5031, 0.4941, 0.5054,\n",
       "        0.5027, 0.4702, 0.4861, 0.5035, 0.5024, 0.5078, 0.4977, 0.4977, 0.5070,\n",
       "        0.4957, 0.4996, 0.5219, 0.5217, 0.4575, 0.5113, 0.5233, 0.5143, 0.4990,\n",
       "        0.4927, 0.4823, 0.5388, 0.5135, 0.5068, 0.4922, 0.5017, 0.5062, 0.5411,\n",
       "        0.5331, 0.5208, 0.4681, 0.5322, 0.5152, 0.4981, 0.4693, 0.4668, 0.5192,\n",
       "        0.4645, 0.5073, 0.4846, 0.4828, 0.5374, 0.5000, 0.4795, 0.5125, 0.5127,\n",
       "        0.5638, 0.4955, 0.4972, 0.5225, 0.4776, 0.5169, 0.5200, 0.5011, 0.5074,\n",
       "        0.4484, 0.5262, 0.4941, 0.5099, 0.4951, 0.4792, 0.5082, 0.4830, 0.5106,\n",
       "        0.5401, 0.5013, 0.5126, 0.4931, 0.5091, 0.5295, 0.5104, 0.5120, 0.4996,\n",
       "        0.4772, 0.4721, 0.4931, 0.4723, 0.5157, 0.5093, 0.5158, 0.4819, 0.5145,\n",
       "        0.4711, 0.4805, 0.4904, 0.4815, 0.5071, 0.4782, 0.5098, 0.5113, 0.5299,\n",
       "        0.4830, 0.4761, 0.4945, 0.4879, 0.5154, 0.4936, 0.4991, 0.5090, 0.5205,\n",
       "        0.5162, 0.4752, 0.4917, 0.5336, 0.5304, 0.5175, 0.5054, 0.4956, 0.5275,\n",
       "        0.5025, 0.4893, 0.5268, 0.4902, 0.4808, 0.4590, 0.4660, 0.5002, 0.5500,\n",
       "        0.5165, 0.5298, 0.5145, 0.4930, 0.4921, 0.4812, 0.4866, 0.5194, 0.5359,\n",
       "        0.5114, 0.4989, 0.4970, 0.5177, 0.4933, 0.4950, 0.5482, 0.5254, 0.5119,\n",
       "        0.5088, 0.5128, 0.5321, 0.4824, 0.4873, 0.5271, 0.4933, 0.5301, 0.4919,\n",
       "        0.4950], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Encoder\"\"\"\n",
    "\n",
    "import diffusers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Data\n",
    "x = np.random.rand(3,2)\n",
    "\n",
    "# 2. Encoder (= encoder + decoder)\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid()) # output between 0 and 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model + loss + optimizer\n",
    "model     = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Ex forward pass with dummy data\n",
    "input_data = torch.randn(1, 784) # input example (1 sample, 784 features)\n",
    "output     = model(input_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 3. Decoder\n",
    "\n",
    "# # 4. Loss\n",
    "# mae_loss = (1/n) * np.sum((x-x)^2)\n",
    "\n",
    "# # 5. Optimization\n",
    "\n",
    "# for i in x:\n",
    "#     loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
